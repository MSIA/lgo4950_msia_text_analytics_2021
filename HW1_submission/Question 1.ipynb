{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "\n",
    "import nltk\n",
    "import spacy \n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.pipeline import Sentencizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the book\n",
    "\n",
    "__License__: https://www.gutenberg.org/policy/license.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1\n",
      "\n",
      "      It is a truth universally acknowledged, that a single man in\n",
      "      possession of a good fortune, must be in want of a wife.\n",
      "\n",
      "      However little known the feelings or views of such a\n"
     ]
    }
   ],
   "source": [
    "with open('Pride and Prejudice - Jane Austen Chapter 1 to 20.txt') as f:\n",
    "    book = f.read()\n",
    "    print(book[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1 It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife. However little known the feelings or views of such a man may be on his f\n"
     ]
    }
   ],
   "source": [
    "book_content_list = book.split()\n",
    "new_book = \" \".join(book_content_list)\n",
    "print(new_book[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199920"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean time: 0.1948823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.202879,\n",
       " 0.184694,\n",
       " 0.17732,\n",
       " 0.176843,\n",
       " 0.213453,\n",
       " 0.207826,\n",
       " 0.198524,\n",
       " 0.188604,\n",
       " 0.19957,\n",
       " 0.19911]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_token_words_time = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    \n",
    "    # Start timer\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    # tokenize words\n",
    "    word_tokens = nltk.word_tokenize(new_book)\n",
    "    \n",
    "    # Stop timer\n",
    "    finish = datetime.datetime.now()\n",
    "    \n",
    "    # Compute time for operation\n",
    "    nltk_token_words_time.append((finish - start).total_seconds())\n",
    "\n",
    "print('mean time: ' + str(np.mean(nltk_token_words_time)))\n",
    "nltk_token_words_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'truth'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(new_book)[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean time: 0.05494140000000001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.060867,\n",
       " 0.055356,\n",
       " 0.054919,\n",
       " 0.054774,\n",
       " 0.054322,\n",
       " 0.061668,\n",
       " 0.059307,\n",
       " 0.051871,\n",
       " 0.048654,\n",
       " 0.047676]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_token_sent_time = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    \n",
    "    # Start timer\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    # tokenize sentences\n",
    "    sentence_tokens = nltk.tokenize.sent_tokenize(new_book)\n",
    "    \n",
    "    # Stop timer\n",
    "    finish = datetime.datetime.now()\n",
    "    \n",
    "    # Compute time for operation\n",
    "    nltk_token_sent_time.append((finish - start).total_seconds())\n",
    "\n",
    "print('mean time: ' + str(np.mean(nltk_token_sent_time)))\n",
    "nltk_token_sent_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“But it is,” returned she; “for Mrs. Long has just been here, and she told me all about it.” Mr. Bennet made no answer.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.tokenize.sent_tokenize(new_book)[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean time: 1.4022461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.508282,\n",
       " 1.451872,\n",
       " 1.574116,\n",
       " 1.449399,\n",
       " 1.496267,\n",
       " 1.323999,\n",
       " 1.329226,\n",
       " 1.305203,\n",
       " 1.30432,\n",
       " 1.279777]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_pos_time = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    \n",
    "    # Start timer\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    # tag words\n",
    "    tags = nltk.pos_tag(nltk.tokenize.word_tokenize(book))\n",
    "    \n",
    "    # Stop timer\n",
    "    finish = datetime.datetime.now()\n",
    "    \n",
    "    # Compute time for operation\n",
    "    nltk_pos_time.append((finish - start).total_seconds())\n",
    "\n",
    "print('mean time: ' + str(np.mean(nltk_pos_time)))\n",
    "nltk_pos_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('is', 'VBZ')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(nltk.tokenize.word_tokenize(book))[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean time: 0.44251510000000005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.63918,\n",
       " 0.339941,\n",
       " 0.321635,\n",
       " 0.300169,\n",
       " 0.315072,\n",
       " 0.305255,\n",
       " 0.301121,\n",
       " 0.312424,\n",
       " 0.293688,\n",
       " 0.296666]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_stemming_time = []\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "for i in range(0,10):\n",
    "    \n",
    "    # Start timer\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    # stemming\n",
    "    word_tokens = nltk.word_tokenize(new_book)\n",
    "    stems = [lemmatizer.lemmatize(w) for w in word_tokens]\n",
    "    \n",
    "    # Stop timer\n",
    "    finish = datetime.datetime.now()\n",
    "    \n",
    "    # Compute time for operation\n",
    "    nltk_stemming_time.append((finish - start).total_seconds())\n",
    "\n",
    "print('mean time: ' + str(np.mean(nltk_stemming_time)))\n",
    "nltk_stemming_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: feelings\n",
      "after: feeling\n"
     ]
    }
   ],
   "source": [
    "print('before: ' + word_tokens[32])\n",
    "print('after: ' +[lemmatizer.lemmatize(w) for w in word_tokens][32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spacy.io/usage/processing-pipelines\n",
    "\n",
    "Use the following command to find which pipes are activated: 'nlp.config'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean time: 0.0551747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.226859,\n",
       " 0.01637,\n",
       " 0.015665,\n",
       " 0.015225,\n",
       " 0.107003,\n",
       " 0.015578,\n",
       " 0.016011,\n",
       " 0.015645,\n",
       " 0.108461,\n",
       " 0.01493]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_token_words_time = []\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "for i in range(0,10):\n",
    "    \n",
    "    # Start timer\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    # tokenize words\n",
    "    tokens = tokenizer(new_book)\n",
    "    words = []\n",
    "    for token in tokens:\n",
    "        words.append(token)\n",
    "\n",
    "    # Stop timer\n",
    "    finish = datetime.datetime.now()\n",
    "    \n",
    "    # Compute time for operation\n",
    "    spacy_token_words_time.append((finish - start).total_seconds())\n",
    "\n",
    "print('mean time: ' + str(np.mean(spacy_token_words_time)))\n",
    "spacy_token_words_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "truth"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "for token in tokens:\n",
    "    words.append(token)\n",
    "words[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x7f95f7d5acc0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sent = spacy.load(\"en_core_web_sm\",disable=['tok2vec',\n",
    "                                           'tagger',\n",
    "                                           'parser',\n",
    "                                           'senter',\n",
    "                                           'attribute_ruler',\n",
    "                                           'lemmatizer',\n",
    "                                           'ner'])\n",
    "nlp_sent.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean time: 0.2916965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.362631,\n",
       " 0.282622,\n",
       " 0.285327,\n",
       " 0.286404,\n",
       " 0.281837,\n",
       " 0.279676,\n",
       " 0.268161,\n",
       " 0.306057,\n",
       " 0.277944,\n",
       " 0.286306]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_token_sent_time = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    \n",
    "    # Start timer\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    # tokenize sentences\n",
    "    doc = nlp_sent(new_book)\n",
    "    sent = []\n",
    "    for sentence in doc.sents:\n",
    "        sent.append(sentence)    \n",
    "        \n",
    "    # Stop timer\n",
    "    finish = datetime.datetime.now()\n",
    "    \n",
    "    # Compute time for operation\n",
    "    spacy_token_sent_time.append((finish - start).total_seconds())\n",
    "\n",
    "print('mean time: ' + str(np.mean(spacy_token_sent_time)))\n",
    "spacy_token_sent_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mr. Bennet made no answer. “"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp_sent(new_book)\n",
    "sent = []\n",
    "for sentence in doc.sents:\n",
    "    sent.append(sentence)   \n",
    "    \n",
    "sent[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_tag = spacy.load(\"en_core_web_sm\",exclude=[\n",
    "                                               #'tokenizer',\n",
    "                                               #'sentencizer'\n",
    "                                               'parser',\n",
    "                                               'senter',\n",
    "                                               'lemmatizer',\n",
    "                                               'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_pos_time = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    \n",
    "    # Start timer\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    # tag words\n",
    "    tags = [(w.pos_) for w in nlp_tag(new_book)]\n",
    "    \n",
    "    # Stop timer\n",
    "    finish = datetime.datetime.now()\n",
    "    \n",
    "    # Compute time for operation\n",
    "    spacy_pos_time.append((finish - start).total_seconds())\n",
    "\n",
    "print('mean time: ' + str(np.mean(spacy_pos_time)))\n",
    "spacy_pos_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [(w, w.pos_) for w in nlp_tag(new_book)]\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_stem = spacy.load(\"en_core_web_sm\",disable=[\n",
    "                                           'parser',\n",
    "                                           'senter',\n",
    "                                           'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_stemming_time = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    \n",
    "    # Start timer\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    # stemming\n",
    "    [w.lemma_ for w in nlp_stem(book)]\n",
    "    \n",
    "    # Stop timer\n",
    "    finish = datetime.datetime.now()\n",
    "    \n",
    "    # Compute time for operation\n",
    "    spacy_stemming_time.append((finish - start).total_seconds())\n",
    "\n",
    "print('mean time: ' + str(np.mean(spacy_stemming_time)))\n",
    "spacy_stemming_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[w,w.lemma_] for w in nlp_stem(new_book)][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[w,w.lemma_] for w in nlp_stem(new_book)][32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_single=spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open('Pride and Prejudice - Jane Austen Chapter 1 to 20.txt') as f:\n",
    "    book1 = f.read()\n",
    "\n",
    "# Start timer\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "# Operation\n",
    "docs = nlp_single(book1)\n",
    "\n",
    "# Stop timer\n",
    "finish = datetime.datetime.now()\n",
    "\n",
    "# Compute time for operation\n",
    "print((finish - start).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_multi=spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open('Pride and Prejudice - Jane Austen Chapter 1 to 20.txt') as f:\n",
    "    book2 = f.read()\n",
    "\n",
    "# Start timer\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "# Operation\n",
    "docs = nlp_multi.pipe(book2, n_process=8)\n",
    "\n",
    "# Stop timer\n",
    "finish = datetime.datetime.now()\n",
    "\n",
    "# Compute time for operation\n",
    "print((finish - start).total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.array([t for t in (np.mean(nltk_token_words_time), np.mean(spacy_token_words_time),\n",
    "                                        np.mean(nltk_token_sent_time), np.mean(spacy_token_sent_time),\n",
    "                                        np.mean(nltk_pos_time), np.mean(spacy_pos_time),\n",
    "                                        nltk_stemming_time[0], np.mean(spacy_stemming_time))]).reshape((4, 2)),\n",
    "             index=[\"Word token\", \"Sentence token\", \"POS tagging\", \"Stemming\"],\n",
    "             columns=[\"NLTK\", \"Spacy\"]\n",
    "            )\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
